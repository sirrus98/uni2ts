{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51be5533-c0e9-4a60-b2f5-c0e7e9f3b7a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T16:03:56.305587Z",
     "start_time": "2024-09-04T16:03:56.287596Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pyarrow.compute as pc\n",
    "import numpy as np\n",
    "\n",
    "from uni2ts.common.env import env\n",
    "from uni2ts.data.builder.lotsa_v1 import (\n",
    "    Buildings900KDatasetBuilder,\n",
    "    BuildingsBenchDatasetBuilder,\n",
    "    CloudOpsTSFDatasetBuilder,\n",
    "    CMIP6DatasetBuilder,\n",
    "    ERA5DatasetBuilder,\n",
    "    GluonTSDatasetBuilder,\n",
    "    LargeSTDatasetBuilder,\n",
    "    LibCityDatasetBuilder,\n",
    "    OthersLOTSADatasetBuilder,\n",
    "    ProEnFoDatasetBuilder,\n",
    "    SubseasonalDatasetBuilder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9657d9c-6371-44dc-8e5f-20dc632a2ef7",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to calculate the dataset weighting in the [pre-training dataset config file](../cli/conf/pretrain/data/lotsa_v1_weighted.yaml). We will see how to automatically generate this file to avoid excessive manual labor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9738e614-8602-4465-b697-cec676e8b2d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T16:03:57.559590Z",
     "start_time": "2024-09-04T16:03:57.545588Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_list = (\n",
    "    Buildings900KDatasetBuilder.dataset_list\n",
    "    + BuildingsBenchDatasetBuilder.dataset_list\n",
    "    + CloudOpsTSFDatasetBuilder.dataset_list\n",
    "    + CMIP6DatasetBuilder.dataset_list\n",
    "    + ERA5DatasetBuilder.dataset_list\n",
    "    + GluonTSDatasetBuilder.dataset_list\n",
    "    + LargeSTDatasetBuilder.dataset_list\n",
    "    + LibCityDatasetBuilder.dataset_list\n",
    "    + OthersLOTSADatasetBuilder.dataset_list\n",
    "    + ProEnFoDatasetBuilder.dataset_list\n",
    "    + SubseasonalDatasetBuilder.dataset_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2694e49-c24a-421b-95c8-f52797f9dceb",
   "metadata": {},
   "source": [
    "1. Obtain the lengths of all time series from all available datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22558fc9-ad7e-415e-a796-98da6faa6d4c",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-09-04T16:03:59.657860Z",
     "start_time": "2024-09-04T16:03:59.471453Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m lengths \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m dataset_list:\n\u001B[1;32m----> 3\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mload_from_disk(\u001B[38;5;28mstr\u001B[39m(\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLOTSA_V1_PATH\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m))\u001B[38;5;241m.\u001B[39mwith_format(\n\u001B[0;32m      4\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      5\u001B[0m     )\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dataset[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m      7\u001B[0m         lengths[name] \u001B[38;5;241m=\u001B[39m pc\u001B[38;5;241m.\u001B[39mlist_value_length(\n\u001B[0;32m      8\u001B[0m             pc\u001B[38;5;241m.\u001B[39mlist_flatten(pc\u001B[38;5;241m.\u001B[39mlist_slice(dataset\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mcolumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m      9\u001B[0m         )\u001B[38;5;241m.\u001B[39mto_numpy()\n",
      "\u001B[1;31mTypeError\u001B[0m: unsupported operand type(s) for /: 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "lengths = {}\n",
    "for name in dataset_list:\n",
    "    dataset = datasets.load_from_disk(str(env.LOTSA_V1_PATH / name)).with_format(\n",
    "        \"numpy\"\n",
    "    )\n",
    "    if dataset[0][\"target\"].ndim > 1:\n",
    "        lengths[name] = pc.list_value_length(\n",
    "            pc.list_flatten(pc.list_slice(dataset.data.column(\"target\"), 0, 1))\n",
    "        ).to_numpy()\n",
    "    else:\n",
    "        lengths[name] = pc.list_value_length(dataset.data.column(\"target\")).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c4a04-3a94-48c9-bf49-f4ea16f8f83a",
   "metadata": {},
   "source": [
    "2. Some datasets have been split into smaller chunks for efficiency -- group them back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f2ba108-9681-4378-ae35-cf45be25ac8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T16:04:15.273105Z",
     "start_time": "2024-09-04T16:04:15.257104Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset_group(name):\n",
    "    if name.startswith(\"era5\"):\n",
    "        return \"era5\"\n",
    "    if name.startswith(\"cmip6\"):\n",
    "        return \"cmip6\"\n",
    "    if name.startswith(\"largest\"):\n",
    "        return \"largest\"\n",
    "    return name\n",
    "\n",
    "\n",
    "group_lengths = {}\n",
    "for k, v in lengths.items():\n",
    "    group = get_dataset_group(k)\n",
    "    if group in group_lengths:\n",
    "        group_lengths[group] = np.concatenate([group_lengths[group], v])\n",
    "    else:\n",
    "        group_lengths[group] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ae20b-45f3-4527-aa26-22e6c628a2be",
   "metadata": {},
   "source": [
    "3. Compute the weights for each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6282f8af-1fbc-400c-a94e-f8d82532bf90",
   "metadata": {},
   "source": [
    "From the [paper](https://arxiv.org/abs/2402.02592), the sampling distribution for dataset $D_k$ is given by:\n",
    "\n",
    "\\begin{align}\n",
    "p(D_k) & = \\frac{\\min(\\omega_k, \\epsilon)}{\\sum_{i=1}^K \\min(\\omega_i, \\epsilon)} \\\\\n",
    "\\\\\n",
    "\\omega_k & = \\frac{|D_k|}{\\sum_i^K |D_i|} \\\\\n",
    "\\\\\n",
    "|D_k| & = \\sum_{i \\in D_k} T_i\n",
    "\\end{align}\n",
    "where $T_i$ for $i \\in D_k$ is the length of time series $i$ belonging to dataset $D_k$.\n",
    "\n",
    "The default PyTorch sampler samples each dataset $D_k$ based on the number of time series in the dataset, i.e. with probability:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\sum_{i \\in D_k} 1}{\\sum_{j=1}^K \\sum_{l \\in D_j} 1}\n",
    "\\end{align}\n",
    "\n",
    "`weight_map` aims to _reweight_ the probability to sample dataset $D_k$ with a multiplier. \n",
    "\n",
    "$\\mathrm{reweight}_k$, which is calculated as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{reweight}_k = \\omega_k * \\frac{\\sum_{j=1}^K \\sum_{l \\in D_j} 1}{\\sum_{i \\in D_k} 1}\n",
    "\\end{align}\n",
    "\n",
    "Thus, the probability to sample dataset $D_k$ is $\\omega_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b46882-eef9-478a-9004-15670b242ece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T16:04:18.394117Z",
     "start_time": "2024-09-04T16:04:18.378116Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute \\omega_k\n",
    "total_lengths = np.asarray([v.sum() for v in group_lengths.values()])\n",
    "omegas = total_lengths / total_lengths.sum()\n",
    "omegas = np.clip(omegas, a_min=0, a_max=0.001)\n",
    "weights = omegas / omegas.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1867dddb-da26-4c28-b05f-3817d4d30d6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T16:04:19.061114Z",
     "start_time": "2024-09-04T16:04:19.051114Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute reweight_k\n",
    "num_ts = np.asarray([v.shape[0] for v in group_lengths.values()])\n",
    "group_reweights = weights * num_ts.sum() / num_ts\n",
    "group_reweights = {k: v for k, v in zip(group_lengths.keys(), group_reweights)}\n",
    "group_size = {\n",
    "    group: len([k for k in lengths.keys() if get_dataset_group(k) == group])\n",
    "    for group in group_lengths.keys()\n",
    "}\n",
    "reweights = {\n",
    "    k: group_reweights[group] / group_size[group]\n",
    "    for k in lengths.keys()\n",
    "    if (group := get_dataset_group(k))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b4d6a5-37c5-4b6a-af8a-3d2309eb3396",
   "metadata": {},
   "source": [
    "4. Finally, we can generate the YAML file required for the pre-training dataset with the appropriate `weight_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "305782c4-8602-4f12-8794-7db18a72010a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T16:04:20.530756Z",
     "start_time": "2024-09-04T16:04:20.509757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- _target_: uni2ts.data.builder.lotsa_v1.Buildings900KDatasetBuilder\n",
      "  datasets: ${cls_getattr:${._target_},dataset_list}\n",
      "  weight_map:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'buildings_900k'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 18\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  weight_map:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m builder_cls\u001B[38;5;241m.\u001B[39mdataset_list:\n\u001B[1;32m---> 18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreweights[dataset]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  sample_time_series:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    _target_: uni2ts.data.dataset.SampleTimeSeriesType\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'buildings_900k'"
     ]
    }
   ],
   "source": [
    "for builder_cls in [\n",
    "    Buildings900KDatasetBuilder,\n",
    "    BuildingsBenchDatasetBuilder,\n",
    "    CloudOpsTSFDatasetBuilder,\n",
    "    CMIP6DatasetBuilder,\n",
    "    ERA5DatasetBuilder,\n",
    "    GluonTSDatasetBuilder,\n",
    "    LargeSTDatasetBuilder,\n",
    "    LibCityDatasetBuilder,\n",
    "    OthersLOTSADatasetBuilder,\n",
    "    ProEnFoDatasetBuilder,\n",
    "    SubseasonalDatasetBuilder,\n",
    "]:\n",
    "    print(f\"- _target_: uni2ts.data.builder.lotsa_v1.{builder_cls.__name__}\")\n",
    "    print(\"  datasets: ${cls_getattr:${._target_},dataset_list}\")\n",
    "    print(\"  weight_map:\")\n",
    "    for dataset in builder_cls.dataset_list:\n",
    "        print(f\"    {dataset}: {reweights[dataset]}\")\n",
    "    print(\"  sample_time_series:\")\n",
    "    print(\"    _target_: uni2ts.data.dataset.SampleTimeSeriesType\")\n",
    "    print('    _args_: [\"proportional\"]')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3535176a5b2131c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
