{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-04T16:52:04.621163Z",
     "start_time": "2024-09-04T16:52:03.518165Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections.abc import Generator\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from datasets import Features, Sequence, Value"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Prepare a univariate dataset for pre-training/fine-tuning\n",
    "In this example, we will see how to use the Hugging Face ```datasets``` library to prepare your custom datasets to use with ```uni2ts```. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f34a16f27f22b43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly, we load our data which comes in the form of a wide dataframe. Here, each column represents a _univariate_ time series."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc1c2c097dec4da4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                          A       B       C       D       E       F       G  \\\n2021-01-01 00:00:00 -1.3378  0.1268 -0.3645 -1.0864 -2.3803 -0.2447  2.2647   \n2021-01-01 01:00:00 -1.6111  0.0926 -0.1364 -1.1613 -2.1421 -0.3477  2.4262   \n2021-01-01 02:00:00 -1.9259 -0.1420  0.1063 -1.0405 -2.1426 -0.3271  2.4434   \n2021-01-01 03:00:00 -1.9184 -0.4930  0.6269 -0.8531 -1.7060 -0.3088  2.4307   \n2021-01-01 04:00:00 -1.9168 -0.5057  0.9419 -0.7666 -1.4287 -0.4284  2.3258   \n\n                          H       I       J  \n2021-01-01 00:00:00 -0.7917  0.7071  1.3763  \n2021-01-01 01:00:00 -0.9609  0.6413  1.2750  \n2021-01-01 02:00:00 -0.9034  0.4323  0.6767  \n2021-01-01 03:00:00 -0.9602  0.3193  0.5150  \n2021-01-01 04:00:00 -1.2504  0.3660  0.1708  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>D</th>\n      <th>E</th>\n      <th>F</th>\n      <th>G</th>\n      <th>H</th>\n      <th>I</th>\n      <th>J</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2021-01-01 00:00:00</th>\n      <td>-1.3378</td>\n      <td>0.1268</td>\n      <td>-0.3645</td>\n      <td>-1.0864</td>\n      <td>-2.3803</td>\n      <td>-0.2447</td>\n      <td>2.2647</td>\n      <td>-0.7917</td>\n      <td>0.7071</td>\n      <td>1.3763</td>\n    </tr>\n    <tr>\n      <th>2021-01-01 01:00:00</th>\n      <td>-1.6111</td>\n      <td>0.0926</td>\n      <td>-0.1364</td>\n      <td>-1.1613</td>\n      <td>-2.1421</td>\n      <td>-0.3477</td>\n      <td>2.4262</td>\n      <td>-0.9609</td>\n      <td>0.6413</td>\n      <td>1.2750</td>\n    </tr>\n    <tr>\n      <th>2021-01-01 02:00:00</th>\n      <td>-1.9259</td>\n      <td>-0.1420</td>\n      <td>0.1063</td>\n      <td>-1.0405</td>\n      <td>-2.1426</td>\n      <td>-0.3271</td>\n      <td>2.4434</td>\n      <td>-0.9034</td>\n      <td>0.4323</td>\n      <td>0.6767</td>\n    </tr>\n    <tr>\n      <th>2021-01-01 03:00:00</th>\n      <td>-1.9184</td>\n      <td>-0.4930</td>\n      <td>0.6269</td>\n      <td>-0.8531</td>\n      <td>-1.7060</td>\n      <td>-0.3088</td>\n      <td>2.4307</td>\n      <td>-0.9602</td>\n      <td>0.3193</td>\n      <td>0.5150</td>\n    </tr>\n    <tr>\n      <th>2021-01-01 04:00:00</th>\n      <td>-1.9168</td>\n      <td>-0.5057</td>\n      <td>0.9419</td>\n      <td>-0.7666</td>\n      <td>-1.4287</td>\n      <td>-0.4284</td>\n      <td>2.3258</td>\n      <td>-1.2504</td>\n      <td>0.3660</td>\n      <td>0.1708</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataframe\n",
    "url_wide = (\n",
    "    \"https://gist.githubusercontent.com/rsnirwan/c8c8654a98350fadd229b00167174ec4\"\n",
    "    \"/raw/a42101c7786d4bc7695228a0f2c8cea41340e18f/ts_wide.csv\"\n",
    ")\n",
    "df = pd.read_csv(url_wide, index_col=0, parse_dates=True)\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T16:52:04.759164Z",
     "start_time": "2024-09-04T16:52:04.622166Z"
    }
   },
   "id": "78f79007648fce3e",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Method 1: Example generator function\n",
    "1. Create an example generator function, a function which yields each individual time series. Each time series consists of \n",
    "    1. target: target time series that should be predicted\n",
    "    2. start: timestamp of the first time step\n",
    "    3. freq: frequency str of time series\n",
    "    4. item_id: identifier \n",
    "    5. (optional) past_feat_dynamic_real: time series for which only the context values are known\n",
    "    6. (optional) feat_dynamic_real: time series for which the context and prediction values are known\n",
    "2. Define the schema for the features to ensure the datasets library saves the correct data types.\n",
    "3. Write the data to disk using the ```from_generator``` function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f1c2c81076c49d0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def example_gen_func() -> Generator[dict[str, Any]]:\n",
    "    for i in range(len(df.columns)):\n",
    "        yield {\n",
    "            \"target\": df.iloc[:, i].to_numpy(),  # array of shape (time,)\n",
    "            \"start\": df.index[0],\n",
    "            \"freq\": pd.infer_freq(df.index),\n",
    "            \"item_id\": f\"item_{i}\",\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T16:52:04.774162Z",
     "start_time": "2024-09-04T16:52:04.760165Z"
    }
   },
   "id": "5896a61701815d0",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "features = Features(\n",
    "    dict(\n",
    "        target=Sequence(Value(\"float32\")),\n",
    "        start=Value(\"timestamp[s]\"),\n",
    "        freq=Value(\"string\"),\n",
    "        item_id=Value(\"string\"),\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T16:52:04.789162Z",
     "start_time": "2024-09-04T16:52:04.775164Z"
    }
   },
   "id": "997f68f06ba2ccab",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "269f0e23097a40f082db949d4e3fe8af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c99c831aada04f2ca5a224c8743bc86e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dataset = datasets.Dataset.from_generator(example_gen_func, features=features)\n",
    "hf_dataset.save_to_disk(Path(\"example_dataset_1\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T16:52:04.995164Z",
     "start_time": "2024-09-04T16:52:04.791164Z"
    }
   },
   "id": "86f35b94bc7a24c5",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Method 2: Sharded example generator function\n",
    "For larger datasets, the Hugging Face ```datasets``` library is able to use multiprocessing to speed up the generation of examples. Since the ```from_generator``` function takes as input a generator object which iterates through every example, naively using this function with multiprocessing does not lead to any speed ups. Instead, we need to provide a _sharded_ generator function, which is able to index into the specific examples based on the inputs. See the following example for a simple recipe:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dd48d9ebaceaf6d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def sharded_example_gen_func(examples: list[int]) -> Generator[dict[str, Any]]:\n",
    "    for i in examples:\n",
    "        yield {\n",
    "            \"target\": df.iloc[:, i].to_numpy(),\n",
    "            \"start\": df.index[0],\n",
    "            \"freq\": pd.infer_freq(df.index),\n",
    "            \"item_id\": f\"item_{i}\",\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T18:40:19.586200Z",
     "start_time": "2024-09-04T18:40:19.574201Z"
    }
   },
   "id": "d671e1f795f13469",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "features = Features(\n",
    "    dict(\n",
    "        target=Sequence(Value(\"float32\")),\n",
    "        start=Value(\"timestamp[s]\"),\n",
    "        freq=Value(\"string\"),\n",
    "        item_id=Value(\"string\"),\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T18:40:19.967210Z",
     "start_time": "2024-09-04T18:40:19.955210Z"
    }
   },
   "id": "c99799fcfd02d50",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "380d847279bc40d6afe5a03e620eba8a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRemoteTraceback\u001B[0m                           Traceback (most recent call last)",
      "\u001B[1;31mRemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\Patrick Xu\\Desktop\\uni2ts\\venv\\lib\\site-packages\\datasets\\builder.py\", line 1726, in _prepare_split_single\n    for key, record in generator:\n  File \"C:\\Users\\Patrick Xu\\Desktop\\uni2ts\\venv\\lib\\site-packages\\datasets\\packaged_modules\\generator\\generator.py\", line 30, in _generate_examples\n    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):\n  File \"C:\\Users\\Patrick Xu\\AppData\\Local\\Temp\\ipykernel_23124\\2800727010.py\", line 4, in sharded_example_gen_func\nNameError: name 'df' is not defined\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Patrick Xu\\Desktop\\uni2ts\\venv\\lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\Patrick Xu\\Desktop\\uni2ts\\venv\\lib\\site-packages\\datasets\\utils\\py_utils.py\", line 625, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"C:\\Users\\Patrick Xu\\Desktop\\uni2ts\\venv\\lib\\site-packages\\datasets\\builder.py\", line 1762, in _prepare_split_single\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mDatasetGenerationError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m hf_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mdatasets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_generator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43msharded_example_gen_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgen_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mexamples\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m hf_dataset\u001B[38;5;241m.\u001B[39msave_to_disk(Path(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexample_dataset_2\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[1;32m~\\Desktop\\uni2ts\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:1074\u001B[0m, in \u001B[0;36mDataset.from_generator\u001B[1;34m(generator, features, cache_dir, keep_in_memory, gen_kwargs, num_proc, **kwargs)\u001B[0m\n\u001B[0;32m   1017\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Create a Dataset from a generator.\u001B[39;00m\n\u001B[0;32m   1018\u001B[0m \n\u001B[0;32m   1019\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1062\u001B[0m \u001B[38;5;124;03m```\u001B[39;00m\n\u001B[0;32m   1063\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1064\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgenerator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GeneratorDatasetInputStream\n\u001B[0;32m   1066\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mGeneratorDatasetInputStream\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1067\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgenerator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1068\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1069\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1070\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkeep_in_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1071\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgen_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgen_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1072\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1073\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m-> 1074\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\uni2ts\\venv\\lib\\site-packages\\datasets\\io\\generator.py:47\u001B[0m, in \u001B[0;36mGeneratorDatasetInputStream.read\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     44\u001B[0m     verification_mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     45\u001B[0m     base_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     50\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverification_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtry_from_hf_gcs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mas_dataset(\n\u001B[0;32m     56\u001B[0m         split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m, verification_mode\u001B[38;5;241m=\u001B[39mverification_mode, in_memory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeep_in_memory\n\u001B[0;32m     57\u001B[0m     )\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dataset\n",
      "File \u001B[1;32m~\\Desktop\\uni2ts\\venv\\lib\\site-packages\\datasets\\builder.py:1005\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001B[0m\n\u001B[0;32m   1003\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_proc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1004\u001B[0m         prepare_split_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_proc\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_proc\n\u001B[1;32m-> 1005\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download_and_prepare(\n\u001B[0;32m   1006\u001B[0m         dl_manager\u001B[38;5;241m=\u001B[39mdl_manager,\n\u001B[0;32m   1007\u001B[0m         verification_mode\u001B[38;5;241m=\u001B[39mverification_mode,\n\u001B[0;32m   1008\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_split_kwargs,\n\u001B[0;32m   1009\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdownload_and_prepare_kwargs,\n\u001B[0;32m   1010\u001B[0m     )\n\u001B[0;32m   1011\u001B[0m \u001B[38;5;66;03m# Sync info\u001B[39;00m\n\u001B[0;32m   1012\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(split\u001B[38;5;241m.\u001B[39mnum_bytes \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39msplits\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[1;32m~\\Desktop\\uni2ts\\venv\\lib\\site-packages\\datasets\\builder.py:1767\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._download_and_prepare\u001B[1;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001B[0m\n\u001B[0;32m   1766\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_download_and_prepare\u001B[39m(\u001B[38;5;28mself\u001B[39m, dl_manager, verification_mode, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_splits_kwargs):\n\u001B[1;32m-> 1767\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m_download_and_prepare(\n\u001B[0;32m   1768\u001B[0m         dl_manager,\n\u001B[0;32m   1769\u001B[0m         verification_mode,\n\u001B[0;32m   1770\u001B[0m         check_duplicate_keys\u001B[38;5;241m=\u001B[39mverification_mode \u001B[38;5;241m==\u001B[39m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS\n\u001B[0;32m   1771\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m verification_mode \u001B[38;5;241m==\u001B[39m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS,\n\u001B[0;32m   1772\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_splits_kwargs,\n\u001B[0;32m   1773\u001B[0m     )\n",
      "File \u001B[1;32m~\\Desktop\\uni2ts\\venv\\lib\\site-packages\\datasets\\builder.py:1100\u001B[0m, in \u001B[0;36mDatasetBuilder._download_and_prepare\u001B[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001B[0m\n\u001B[0;32m   1096\u001B[0m split_dict\u001B[38;5;241m.\u001B[39madd(split_generator\u001B[38;5;241m.\u001B[39msplit_info)\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1099\u001B[0m     \u001B[38;5;66;03m# Prepare split will record examples associated to the split\u001B[39;00m\n\u001B[1;32m-> 1100\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_split(split_generator, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_split_kwargs)\n\u001B[0;32m   1101\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1102\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[0;32m   1103\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot find data file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1104\u001B[0m         \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_download_instructions \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1105\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mOriginal error:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1106\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(e)\n\u001B[0;32m   1107\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\uni2ts\\venv\\lib\\site-packages\\datasets\\builder.py:1634\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._prepare_split\u001B[1;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001B[0m\n\u001B[0;32m   1632\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Pool(num_proc) \u001B[38;5;28;01mas\u001B[39;00m pool:\n\u001B[0;32m   1633\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m pbar:\n\u001B[1;32m-> 1634\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m job_id, done, content \u001B[38;5;129;01min\u001B[39;00m iflatmap_unordered(\n\u001B[0;32m   1635\u001B[0m             pool, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_split_single, kwargs_iterable\u001B[38;5;241m=\u001B[39mkwargs_per_job\n\u001B[0;32m   1636\u001B[0m         ):\n\u001B[0;32m   1637\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[0;32m   1638\u001B[0m                 \u001B[38;5;66;03m# the content is the result of the job\u001B[39;00m\n\u001B[0;32m   1639\u001B[0m                 (\n\u001B[0;32m   1640\u001B[0m                     examples_per_job[job_id],\n\u001B[0;32m   1641\u001B[0m                     bytes_per_job[job_id],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1644\u001B[0m                     shard_lengths_per_job[job_id],\n\u001B[0;32m   1645\u001B[0m                 ) \u001B[38;5;241m=\u001B[39m content\n",
      "File \u001B[1;32m~\\Desktop\\uni2ts\\venv\\lib\\site-packages\\datasets\\utils\\py_utils.py:665\u001B[0m, in \u001B[0;36miflatmap_unordered\u001B[1;34m(pool, func, kwargs_iterable)\u001B[0m\n\u001B[0;32m    662\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    663\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m pool_changed:\n\u001B[0;32m    664\u001B[0m         \u001B[38;5;66;03m# we get the result in case there's an error to raise\u001B[39;00m\n\u001B[1;32m--> 665\u001B[0m         [async_result\u001B[38;5;241m.\u001B[39mget(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.05\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m async_result \u001B[38;5;129;01min\u001B[39;00m async_results]\n",
      "File \u001B[1;32m~\\Desktop\\uni2ts\\venv\\lib\\site-packages\\datasets\\utils\\py_utils.py:665\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    662\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    663\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m pool_changed:\n\u001B[0;32m    664\u001B[0m         \u001B[38;5;66;03m# we get the result in case there's an error to raise\u001B[39;00m\n\u001B[1;32m--> 665\u001B[0m         [\u001B[43masync_result\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.05\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m async_result \u001B[38;5;129;01min\u001B[39;00m async_results]\n",
      "File \u001B[1;32m~\\Desktop\\uni2ts\\venv\\lib\\site-packages\\multiprocess\\pool.py:774\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    772\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n\u001B[0;32m    773\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 774\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n",
      "\u001B[1;31mDatasetGenerationError\u001B[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "hf_dataset = datasets.Dataset.from_generator(\n",
    "    sharded_example_gen_func,\n",
    "    features=features,\n",
    "    gen_kwargs={\"examples\": [i for i in range(len(df.columns))]},\n",
    "    num_proc=2,\n",
    ")\n",
    "hf_dataset.save_to_disk(Path(\"example_dataset_2\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T18:40:22.416890Z",
     "start_time": "2024-09-04T18:40:20.523893Z"
    }
   },
   "id": "f9c72d1c2f52d98f",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Prepare a multivariate dataset for pre-training/fine-tuning\n",
    "Finally, we can also prepare _multivariate_ time series:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da2f641ee996d4b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                          A       B       C       D       E       F       G  \\\n2021-01-01 00:00:00 -1.3378  0.1268 -0.3645 -1.0864 -2.3803 -0.2447  2.2647   \n2021-01-01 01:00:00 -1.6111  0.0926 -0.1364 -1.1613 -2.1421 -0.3477  2.4262   \n2021-01-01 02:00:00 -1.9259 -0.1420  0.1063 -1.0405 -2.1426 -0.3271  2.4434   \n2021-01-01 03:00:00 -1.9184 -0.4930  0.6269 -0.8531 -1.7060 -0.3088  2.4307   \n2021-01-01 04:00:00 -1.9168 -0.5057  0.9419 -0.7666 -1.4287 -0.4284  2.3258   \n\n                          H       I       J  \n2021-01-01 00:00:00 -0.7917  0.7071  1.3763  \n2021-01-01 01:00:00 -0.9609  0.6413  1.2750  \n2021-01-01 02:00:00 -0.9034  0.4323  0.6767  \n2021-01-01 03:00:00 -0.9602  0.3193  0.5150  \n2021-01-01 04:00:00 -1.2504  0.3660  0.1708  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>D</th>\n      <th>E</th>\n      <th>F</th>\n      <th>G</th>\n      <th>H</th>\n      <th>I</th>\n      <th>J</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2021-01-01 00:00:00</th>\n      <td>-1.3378</td>\n      <td>0.1268</td>\n      <td>-0.3645</td>\n      <td>-1.0864</td>\n      <td>-2.3803</td>\n      <td>-0.2447</td>\n      <td>2.2647</td>\n      <td>-0.7917</td>\n      <td>0.7071</td>\n      <td>1.3763</td>\n    </tr>\n    <tr>\n      <th>2021-01-01 01:00:00</th>\n      <td>-1.6111</td>\n      <td>0.0926</td>\n      <td>-0.1364</td>\n      <td>-1.1613</td>\n      <td>-2.1421</td>\n      <td>-0.3477</td>\n      <td>2.4262</td>\n      <td>-0.9609</td>\n      <td>0.6413</td>\n      <td>1.2750</td>\n    </tr>\n    <tr>\n      <th>2021-01-01 02:00:00</th>\n      <td>-1.9259</td>\n      <td>-0.1420</td>\n      <td>0.1063</td>\n      <td>-1.0405</td>\n      <td>-2.1426</td>\n      <td>-0.3271</td>\n      <td>2.4434</td>\n      <td>-0.9034</td>\n      <td>0.4323</td>\n      <td>0.6767</td>\n    </tr>\n    <tr>\n      <th>2021-01-01 03:00:00</th>\n      <td>-1.9184</td>\n      <td>-0.4930</td>\n      <td>0.6269</td>\n      <td>-0.8531</td>\n      <td>-1.7060</td>\n      <td>-0.3088</td>\n      <td>2.4307</td>\n      <td>-0.9602</td>\n      <td>0.3193</td>\n      <td>0.5150</td>\n    </tr>\n    <tr>\n      <th>2021-01-01 04:00:00</th>\n      <td>-1.9168</td>\n      <td>-0.5057</td>\n      <td>0.9419</td>\n      <td>-0.7666</td>\n      <td>-1.4287</td>\n      <td>-0.4284</td>\n      <td>2.3258</td>\n      <td>-1.2504</td>\n      <td>0.3660</td>\n      <td>0.1708</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataframe\n",
    "url_wide = (\n",
    "    \"https://gist.githubusercontent.com/rsnirwan/c8c8654a98350fadd229b00167174ec4\"\n",
    "    \"/raw/a42101c7786d4bc7695228a0f2c8cea41340e18f/ts_wide.csv\"\n",
    ")\n",
    "df = pd.read_csv(url_wide, index_col=0, parse_dates=True)\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T18:40:25.566989Z",
     "start_time": "2024-09-04T18:40:25.461989Z"
    }
   },
   "id": "4c04c75fe20ee30f",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def multivar_example_gen_func() -> Generator[dict[str, Any], None, None]:\n",
    "    yield {\n",
    "        \"target\": df.to_numpy().T,  # array of shape (var, time)\n",
    "        \"start\": df.index[0],\n",
    "        \"freq\": pd.infer_freq(df.index),\n",
    "        \"item_id\": \"item_0\",\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T18:40:26.334603Z",
     "start_time": "2024-09-04T18:40:26.323601Z"
    }
   },
   "id": "33a1928b6f2675c4",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "features = Features(\n",
    "    dict(\n",
    "        target=Sequence(\n",
    "            Sequence(Value(\"float32\")), length=len(df.columns)\n",
    "        ),  # multivariate time series are saved as (var, time)\n",
    "        start=Value(\"timestamp[s]\"),\n",
    "        freq=Value(\"string\"),\n",
    "        item_id=Value(\"string\"),\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T18:40:27.248601Z",
     "start_time": "2024-09-04T18:40:27.231601Z"
    }
   },
   "id": "310a3121cac37e91",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f8f923b101c4817bf3b260a631df9af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d6df34658234260b2986db36ad10c52"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dataset = datasets.Dataset.from_generator(\n",
    "    multivar_example_gen_func, features=features\n",
    ")\n",
    "hf_dataset.save_to_disk(\"example_dataset_multi\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T18:40:28.213603Z",
     "start_time": "2024-09-04T18:40:28.158602Z"
    }
   },
   "id": "a85ece72e58e73ec",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Inspecting the processed data\n",
    "Let's inspect the processed datasets to ensure that our data has been processed correctly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94516da2a373173"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory example_dataset_2 not found",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Load datasets with ArrowTableIndexer\u001B[39;00m\n\u001B[0;32m      2\u001B[0m ds1 \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mload_from_disk(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexample_dataset_1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mwith_format(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m ds2 \u001B[38;5;241m=\u001B[39m \u001B[43mdatasets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_from_disk\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mexample_dataset_2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mwith_format(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      4\u001B[0m ds_multi \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mload_from_disk(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexample_dataset_multi\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mwith_format(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\uni2ts\\venv\\lib\\site-packages\\datasets\\load.py:2655\u001B[0m, in \u001B[0;36mload_from_disk\u001B[1;34m(dataset_path, fs, keep_in_memory, storage_options)\u001B[0m\n\u001B[0;32m   2653\u001B[0m fs, _, _ \u001B[38;5;241m=\u001B[39m fsspec\u001B[38;5;241m.\u001B[39mget_fs_token_paths(dataset_path, storage_options\u001B[38;5;241m=\u001B[39mstorage_options)\n\u001B[0;32m   2654\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fs\u001B[38;5;241m.\u001B[39mexists(dataset_path):\n\u001B[1;32m-> 2655\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDirectory \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   2656\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fs\u001B[38;5;241m.\u001B[39misfile(posixpath\u001B[38;5;241m.\u001B[39mjoin(dataset_path, config\u001B[38;5;241m.\u001B[39mDATASET_INFO_FILENAME)) \u001B[38;5;129;01mand\u001B[39;00m fs\u001B[38;5;241m.\u001B[39misfile(\n\u001B[0;32m   2657\u001B[0m     posixpath\u001B[38;5;241m.\u001B[39mjoin(dataset_path, config\u001B[38;5;241m.\u001B[39mDATASET_STATE_JSON_FILENAME)\n\u001B[0;32m   2658\u001B[0m ):\n\u001B[0;32m   2659\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Dataset\u001B[38;5;241m.\u001B[39mload_from_disk(dataset_path, keep_in_memory\u001B[38;5;241m=\u001B[39mkeep_in_memory, storage_options\u001B[38;5;241m=\u001B[39mstorage_options)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: Directory example_dataset_2 not found"
     ]
    }
   ],
   "source": [
    "# Load datasets with ArrowTableIndexer\n",
    "ds1 = datasets.load_from_disk(\"example_dataset_1\").with_format(\"numpy\")\n",
    "ds2 = datasets.load_from_disk(\"example_dataset_2\").with_format(\"numpy\")\n",
    "ds_multi = datasets.load_from_disk(\"example_dataset_multi\").with_format(\"numpy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T18:40:30.466729Z",
     "start_time": "2024-09-04T18:40:30.369731Z"
    }
   },
   "id": "d38695048216c6b7",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "```example_dataset_1``` and ```example_dataset_2``` are univariate datasets, which should have 10 time series each, and ```example_dataset_multi``` should be a single multivariate time series (with 10 variates). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a4b8a2ce586ac80"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(ds1), len(ds2), len(ds_multi)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-04T16:52:07.782795Z"
    }
   },
   "id": "25ddb7ccd8cc92cb",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspecting the features returned when we index into a time series from the dataset..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7dc1ad99802d749"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ds1[0].keys(), ds2[0].keys(), ds_multi[0].keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-04T16:52:07.783796Z"
    }
   },
   "id": "525d1b0d246d6441",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We should get 2 univariate and 1 multivariate target time series..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8f2b9d043aacf81"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ds1[0][\"target\"].shape, ds2[0][\"target\"].shape, ds_multi[0][\"target\"].shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ee8290adf1e5475",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
